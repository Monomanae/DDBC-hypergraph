{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from scipy.sparse import load_npz,save_npz,diags,csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from tempfile import NamedTemporaryFile\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import gseapy as gp\n",
    "import mygene\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "from collections import deque\n",
    "from goatools.obo_parser import GODag\n",
    "import math\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from gseapy.parser import read_gmt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', None)      # No line-wrapping\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Loading variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISEASE = \"BIPOLAR\"\n",
    "DISEASE_FOLDER = f\"../output/{DISEASE}/\"\n",
    "RESULT_FOLDER = DISEASE_FOLDER + \"leiden_results\"\n",
    "DGIDB_DIRECTORY = f\"../../Gen_Hypergraph/output/DGIDB_{DISEASE}/\"\n",
    "MSIGDB_DIRECTORY = \"../../Gen_Hypergraph/output/MSigDB_Full/\"\n",
    "RESULT_COMMUNITIES_SELECTED = \"result_communities_agg\"\n",
    "RESULT_GRAPH = \"result_graph_agg\"\n",
    "\n",
    "with open(DISEASE_FOLDER + \"gene_to_index_distinct.json\", \"r\") as file:\n",
    "    gene_to_index_distinct = json.load(file)\n",
    "    \n",
    "try:\n",
    "    with open(DGIDB_DIRECTORY + f\"gene_to_index_{DISEASE}.json\", \"r\") as file:\n",
    "        DGIDB_gene_to_index = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    DGIDB_gene_to_index = {}\n",
    "    print(\"File not found. Setting DGIDB_gene_to_index to be {}.\")\n",
    "    \n",
    "    \n",
    "sim_mat = load_npz(f\"{DISEASE_FOLDER}/agg_sim_mat.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_gene_distinct = {v: k for k, v in gene_to_index_distinct.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading result graph and communities\n",
    "with open(f\"{RESULT_FOLDER}/{RESULT_COMMUNITIES}.pkl\", \"rb\") as f:\n",
    "    communities_selected = pickle.load(f)\n",
    "with open(f\"{RESULT_FOLDER}/{RESULT_GRAPH}.pkl\", \"rb\") as f:\n",
    "    graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Helpful functions (big object, drop NAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful functions\n",
    "def drop_nan_from_communities(communities):\n",
    "    cleaned_communities = []\n",
    "    total_dropped = 0\n",
    "\n",
    "    for i, community in enumerate(communities):\n",
    "        cleaned = []\n",
    "        dropped = 0\n",
    "        for g in community:\n",
    "            if g is None or (isinstance(g, float) and math.isnan(g)):\n",
    "                dropped += 1\n",
    "            else:\n",
    "                cleaned.append(g)\n",
    "        cleaned_communities.append(cleaned)\n",
    "        total_dropped += dropped\n",
    "        print(f\"Community {i}: dropped {dropped} NaN entries\")\n",
    "\n",
    "    print(f\"\\nTotal dropped across all communities: {total_dropped}\")\n",
    "    return cleaned_communities\n",
    "\n",
    "def big_objects(n=10, min_mb=1):\n",
    "    \"\"\"\n",
    "    Show the largest objects currently in memory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of top objects to show.\n",
    "    min_mb : float\n",
    "        Minimum size (in MB) to include.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import scipy.sparse as sp\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    def get_size(obj):\n",
    "        try:\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.nbytes\n",
    "            elif isinstance(obj, pd.DataFrame) or isinstance(obj, pd.Series):\n",
    "                return obj.memory_usage(deep=True).sum()\n",
    "            elif sp.issparse(obj):\n",
    "                return (obj.data.nbytes +\n",
    "                        obj.indptr.nbytes +\n",
    "                        obj.indices.nbytes)\n",
    "            else:\n",
    "                return sys.getsizeof(obj)\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    ip = get_ipython()\n",
    "    if ip is None:\n",
    "        ns = globals()\n",
    "    else:\n",
    "        ns = ip.user_ns\n",
    "\n",
    "    items = []\n",
    "    for name, val in ns.items():\n",
    "        if name.startswith('_'):\n",
    "            continue  # skip internals\n",
    "        size = get_size(val)\n",
    "        if size > min_mb * 1024 ** 2:\n",
    "            items.append((name, type(val).__name__, size))\n",
    "\n",
    "    items.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    print(f\"{'Variable':30s} {'Type':25s} {'Size (MB)':>10s}\")\n",
    "    print(\"-\" * 70)\n",
    "    for name, t, size in items[:n]:\n",
    "        print(f\"{name:30s} {t:25s} {size / 1024 ** 2:10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Index to HGNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert index to ncbi\n",
    "def index_to_ncbi(comms,index_to_ncbi = index_to_gene_distinct):\n",
    "    comms_ncbi = [list(map(index_to_ncbi.get, c)) for c in comms]\n",
    "    return comms_ncbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_ncbi = index_to_ncbi(communities_selected,index_to_gene_distinct)\n",
    "print(communities_ncbi)\n",
    "print(len(communities_ncbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCBI to HGNC symbol\n",
    "def ncbi_to_HGNC(comms_ncbi):\n",
    "    comms_HGNC = []\n",
    "    for community in comms_ncbi:\n",
    "        mg = mygene.MyGeneInfo()\n",
    "        entrez_ids = [str(e) for e in community]\n",
    "\n",
    "        results = mg.querymany(\n",
    "            entrez_ids,\n",
    "            scopes=\"entrezgene\",\n",
    "            fields=\"symbol\",\n",
    "            species=\"human\"\n",
    "        )\n",
    "\n",
    "        # Build a mapping: input ID -> symbol (or None)\n",
    "        id_to_symbol = {}\n",
    "        for r in results:\n",
    "            q = str(r.get(\"query\"))\n",
    "            id_to_symbol[q] = r.get(\"symbol\") if not r.get(\"notfound\") else None\n",
    "\n",
    "        # Preserve original order\n",
    "        symbols = [id_to_symbol.get(str(e), None) for e in entrez_ids]\n",
    "        comms_HGNC.append(symbols)\n",
    "    return comms_HGNC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMUNITIES_HGNC = ncbi_to_HGNC(communities_ncbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(COMMUNITIES_HGNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMUNITIES_HGNC = drop_nan_from_communities(COMMUNITIES_HGNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_selected_comm = len(COMMUNITIES_HGNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_selected_comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Categoization Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### GO-slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"../../data\"\n",
    "GO_OBO = f\"{DATA_DIRECTORY}/GO/go-basic.obo\"            # put the file in your working dir (or give full path)\n",
    "GOSLIM_OBO = f\"{DATA_DIRECTORY}/GO/goslim_generic.obo\"  # swap to another slim if you prefer\n",
    "GOSLIM_PIR_OBO = f\"{DATA_DIRECTORY}/GO/goslim_pir.obo\"  # swap to another slim if you prefer\n",
    "GOSLIM_YEAST_OBO = f\"{DATA_DIRECTORY}/GO/goslim_yeast.obo\"\n",
    "GOSLIM_AGR_OBO = f\"{DATA_DIRECTORY}/GO/goslim_agr.obo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO library\n",
    "go = GODag(GO_OBO)\n",
    "\n",
    "# SLIM libraries\n",
    "slim = GODag(GOSLIM_OBO)\n",
    "slim_pir = GODag(GOSLIM_PIR_OBO)\n",
    "slim_yeast = GODag(GOSLIM_YEAST_OBO)\n",
    "slim_agr = GODag(GOSLIM_AGR_OBO)\n",
    "\n",
    "slim_ids = set(slim.keys())\n",
    "slim_pir_ids = set(slim_pir.keys())\n",
    "slim_yeast_ids = set(slim_yeast.keys())\n",
    "slim_agr_ids = set(slim_agr.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_RE = re.compile(r\"(GO:\\d{7})\")\n",
    "\n",
    "def get_goid(term: str):\n",
    "    if isinstance(term, str):\n",
    "        m = GO_RE.search(term)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "    raise RuntimeError(\"Term not found!!\")\n",
    "\n",
    "def get_go_ancestors(go_id):\n",
    "    \"\"\"Return a list of ancestor GO term IDs for the given GO ID using QuickGO.\"\"\"\n",
    "    url = f\"https://www.ebi.ac.uk/QuickGO/services/ontology/go/terms/{go_id}/ancestors\"\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    data = r.json()\n",
    "    results = data.get(\"results\", [])\n",
    "    if not results:\n",
    "        return []\n",
    "\n",
    "    # Ancestors come back as a simple list of GO IDs (strings)\n",
    "    ancestors = results[0].get(\"ancestors\", [])\n",
    "    return set(ancestors)\n",
    "\n",
    "\n",
    "def get_go_ancestors_in_slim(go_id):\n",
    "    ancestors = get_go_ancestors(go_id)\n",
    "    return slim_ids & ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_go_ancestors_at_depth(go_id, depth, include_relations=(\"is_a\", \"part_of\")):\n",
    "    \"\"\"\n",
    "    Return the set of GO term IDs that are ancestors of `go_id` and have\n",
    "    absolute depth == `depth` in the GO DAG.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    go_id : str\n",
    "        Starting GO term (e.g., \"GO:0051310\").\n",
    "    depth : int\n",
    "        Absolute depth in the GO DAG (e.g., 3 means all ancestors at depth=3).\n",
    "    include_relations : tuple[str]\n",
    "        Relation types to traverse upward, e.g. (\"is_a\", \"part_of\", \"regulates\", ...).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    set[str]\n",
    "        Ancestor GO IDs whose term.depth == `depth`. Empty set if none.\n",
    "    \"\"\"\n",
    "    if depth < 0:\n",
    "        return set()\n",
    "    if go_id not in go:\n",
    "        return set()\n",
    "\n",
    "    # One-hop function honoring relation filter\n",
    "    def parent_ids(term):\n",
    "        ids = set()\n",
    "        if \"is_a\" in include_relations:\n",
    "            # GOATOOLS usually puts is_a parents here (and sometimes part_of merged)\n",
    "            ids.update(p.id for p in term.parents)\n",
    "\n",
    "        rel = getattr(term, \"relationship\", {}) or {}\n",
    "        for r in include_relations:\n",
    "            # relationship entries are already GO IDs\n",
    "            ids.update(rel.get(r, []))\n",
    "\n",
    "        # ensure IDs exist in DAG\n",
    "        return {pid for pid in ids if pid in go}\n",
    "\n",
    "    result = set()\n",
    "    frontier = {go_id}\n",
    "    visited = {go_id}\n",
    "\n",
    "    # BFS upwards, but pruning branches that are already above the target depth\n",
    "    while frontier:\n",
    "        next_frontier = set()\n",
    "        for node in frontier:\n",
    "            for pid in parent_ids(go[node]):\n",
    "                if pid in visited:\n",
    "                    continue\n",
    "                visited.add(pid)\n",
    "                d = go[pid].depth  # absolute depth in DAG\n",
    "\n",
    "                if d == depth:\n",
    "                    # ancestor at the exact target depth\n",
    "                    result.add(pid)\n",
    "                elif d > depth:\n",
    "                    # still \"below\" target depth (further from root),\n",
    "                    # its parents might reach the target depth\n",
    "                    next_frontier.add(pid)\n",
    "                # if d < depth: this branch has gone above the target,\n",
    "                # and all further ancestors will have depth <= d, so we can skip\n",
    "        frontier = next_frontier\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### KEGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kegg_name_to_id(species=\"hsa\"):\n",
    "    \"\"\"Map KEGG pathway name -> 'hsaXXXXX' (species-specific).\"\"\"\n",
    "    lines = requests.get(f\"https://rest.kegg.jp/list/pathway/{species}\").text.strip().splitlines()\n",
    "    name_to_id = {}\n",
    "    for ln in lines:\n",
    "        pid, raw = ln.split(\"\\t\")\n",
    "        pid = pid.replace(\"path:\", \"\")  # e.g. hsa03010\n",
    "        # strip \" - Homo sapiens (human)\" suffix\n",
    "        name = re.sub(r\"\\s*-\\s*Homo sapiens.*$\", \"\", raw).strip()\n",
    "        name_to_id[name.lower()] = pid\n",
    "    return name_to_id\n",
    "\n",
    "name_to_id = build_kegg_name_to_id(\"hsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kegg_level2(hsa_id: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Return the KEGG Level 2 category for a pathway like 'hsa03040'.\n",
    "    Example: get_kegg_level2(\"hsa03040\") -> 'Transcription'\n",
    "    \"\"\"\n",
    "    url = f\"http://rest.kegg.jp/get/{hsa_id}\"\n",
    "    try:\n",
    "        text = requests.get(url, timeout=10).text\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith(\"CLASS\"):\n",
    "            # CLASS line looks like: CLASS       Genetic Information Processing; Transcription\n",
    "            parts = [p.strip() for p in line.split(\";\", maxsplit=2)]\n",
    "            if len(parts) >= 2:\n",
    "                return [parts[1]]\n",
    "            elif len(parts) == 1:\n",
    "                return [parts[0].replace(\"CLASS\", \"\").strip()]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Reactome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reactome_level_map(level=1, species=\"9606\"):\n",
    "    \"\"\"\n",
    "    Returns { 'R-HSA-xxxxx': ['CategoryNameAtLevel', ...], ... } for the given species.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    level : int, default=1\n",
    "        1-based depth in the Reactome pathway hierarchy:\n",
    "          - level=1 â†’ top-level Reactome categories (original behavior)\n",
    "          - level=2 â†’ second-level ancestors, etc.\n",
    "        If a node is shallower than `level`, the deepest available ancestor\n",
    "        is used as a fallback.\n",
    "    species : str, default=\"9606\"\n",
    "        Taxonomy ID (\"9606\") or species name (\"Homo sapiens\").\n",
    "    \"\"\"\n",
    "    if level < 1:\n",
    "        raise ValueError(\"level must be >= 1 (1-based depth)\")\n",
    "\n",
    "    # ensure spaces are encoded if a name is used\n",
    "    species_path = species.replace(\" \", \"+\")\n",
    "    url = f\"https://reactome.org/ContentService/data/eventsHierarchy/{species_path}\"\n",
    "    r = requests.get(url, headers={\"Accept\": \"application/json\"}, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    trees = r.json()  # list of trees, one per TopLevelPathway\n",
    "\n",
    "    mapping = {}\n",
    "\n",
    "    def walk(node, ancestors):\n",
    "        \"\"\"\n",
    "        node: current node dict\n",
    "        ancestors: list of ancestor nodes from root to parent of `node`\n",
    "        \"\"\"\n",
    "        # ancestors_chain includes current node at the end\n",
    "        ancestors_chain = ancestors + [node]\n",
    "\n",
    "        st_id = node.get(\"stId\")\n",
    "        if st_id:\n",
    "            # We want the ancestor at depth `level` (1-based).\n",
    "            # If the path is shorter than `level`, fall back to the deepest one.\n",
    "            if len(ancestors_chain) >= level:\n",
    "                cat_node = ancestors_chain[level - 1]\n",
    "            else:\n",
    "                cat_node = ancestors_chain[-1]\n",
    "\n",
    "            cat_name = cat_node.get(\"name\")\n",
    "            if cat_name:\n",
    "                mapping.setdefault(st_id, set()).add(cat_name)\n",
    "\n",
    "        # Recurse into children\n",
    "        for child in node.get(\"children\", []):\n",
    "            walk(child, ancestors_chain)\n",
    "\n",
    "    # Each tree is a top-level pathway\n",
    "    for top in trees:\n",
    "        walk(top, [])\n",
    "\n",
    "    # sets -> sorted lists\n",
    "    return {k: sorted(v) for k, v in mapping.items()}\n",
    "\n",
    "# Example:\n",
    "reactome_level1 = build_reactome_level_map(level = 2)\n",
    "  # -> ['Signal Transduction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reactome_level1[\"R-HSA-9007101\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# Run Enrichment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_SCORE_CAP = 0.001\n",
    "PERCENTAGE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO Analysis; save terms with small size and high p-value\n",
    "def go_enrichment(communities,\n",
    "                  term_score_cap,\n",
    "                  percentage, \n",
    "                  slim_ids = slim_yeast_ids,\n",
    "                  depth = 1):\n",
    "    important_terms = pd.DataFrame(columns=[\"Community Index\",\"Community Size\",\"Term\", \"Overlap\", \"Adjusted P-value\",\"Category\"])\n",
    "    category_counts_and_overlap_score_list = {}\n",
    "    i = 0\n",
    "    num_nonzero_communities = 0\n",
    "    \n",
    "    for community in communities:\n",
    "        # Gene Ontology enrichment\n",
    "        enr_go = gp.enrichr(\n",
    "            gene_list=community,\n",
    "            gene_sets=['GO_Biological_Process_2023',\n",
    "                    'GO_Molecular_Function_2023',\n",
    "                    'GO_Cellular_Component_2023'],\n",
    "            organism='Human',\n",
    "            outdir=None # don't write to disk\n",
    "        )\n",
    "        go_df = enr_go.results\n",
    "        \n",
    "\n",
    "        # Filter by overlap percentage and adjusted p-value\n",
    "        mask =  (go_df[\"Adjusted P-value\"] < term_score_cap) & (go_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > percentage))\n",
    "        filtered = go_df[mask].copy()\n",
    "        \n",
    "        # Categorization from GO-Slim\n",
    "        filtered[\"GO_ID\"] = filtered[\"Term\"].apply(get_goid)\n",
    "        # filtered[\"Slim_IDs\"] = filtered[\"GO_ID\"].apply(get_go_ancestors_in_slim)\n",
    "        filtered[\"Slim_IDs\"] = filtered[\"GO_ID\"].apply(lambda id: get_go_ancestors_at_depth(id, depth=depth, include_relations=(\"is_a\", \"part_of\")))\n",
    "        \n",
    "        # Get empty count\n",
    "        empty_count = (filtered[\"Slim_IDs\"].apply(len) == 0).sum()\n",
    "        \n",
    "        # Get slim names    \n",
    "        filtered[\"Category\"] = filtered[\"Slim_IDs\"].apply(lambda ids: [go[i].name for i in ids])\n",
    "        \n",
    "        # Sort\n",
    "        filtered['Overlap (value)'] = filtered['Overlap'].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]))\n",
    "        filtered = filtered.sort_values(['Overlap (value)'], ascending=False)\n",
    "        \n",
    "        # Compute overlap score for every category:\n",
    "        filtered_exploded = filtered.explode('Category').reset_index(drop=True)\n",
    "        category_counts_and_overlap_score = {}\n",
    "        for val, group in filtered_exploded.groupby('Category'):\n",
    "            overlap_list = group[\"Overlap\"].tolist()\n",
    "            numerators = [(lambda x: int(x.split(\"/\")[0]))(e) for e in overlap_list]\n",
    "            denominators = [(lambda x: int(x.split(\"/\")[1]))(e) for e in overlap_list]\n",
    "            overlap_score = sum(numerators)/sum(denominators)\n",
    "            \n",
    "            category_counts_and_overlap_score[val] = (len(group),overlap_score,)\n",
    "        \n",
    "        category_counts_and_overlap_score_list[i] = category_counts_and_overlap_score\n",
    "        \n",
    "        # Add results to important terms\n",
    "        if not filtered.empty:\n",
    "            # print size of community\n",
    "            print(f\"Size of community: {len(community)}\")\n",
    "            \n",
    "            # print number of filtered terms\n",
    "            print(f\"Number of filtered terms: {len(filtered)}\")\n",
    "            print(f\"Number of unmapped terms: {empty_count}\")      \n",
    "            print(category_counts_and_overlap_score)\n",
    "            filtered.loc[:, \"Community Index\"] = i\n",
    "            filtered.loc[:, \"Community Size\"] = len(community)\n",
    "            important_terms = pd.concat([important_terms, filtered], ignore_index=True)\n",
    "            display(HTML(filtered[[\"Community Index\",'Term','Overlap','Adjusted P-value',\"Slim_IDs\",\"Category\"]].head(10).to_html(max_cols=None)))\n",
    "            num_nonzero_communities += 1\n",
    "\n",
    "        i += 1\n",
    "    print(f\"{num_nonzero_communities} out of {len(communities)} communities had significant GO terms.\")\n",
    "    return important_terms,category_counts_and_overlap_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_important_terms,go_category_counts_and_overlap_score = go_enrichment(COMMUNITIES_HGNC,TERM_SCORE_CAP,PERCENTAGE,slim_ids,depth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_important_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### KEGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEGG\n",
    "def kegg_enrichment(communities,\n",
    "                    term_score_cap,\n",
    "                    percentage):\n",
    "    important_terms = pd.DataFrame(columns=[\"Community Index\",\"Community Size\",\"Term\", \"Overlap\", \"Adjusted P-value\",\"Category\"])\n",
    "    category_counts_and_overlap_score_list = {}\n",
    "    i = 0\n",
    "    num_nonzero_communities = 0\n",
    "    for community in communities:\n",
    "        enr_path = gp.enrichr(\n",
    "            gene_list=community,\n",
    "            gene_sets=['KEGG_2021_Human'],\n",
    "            organism='Human',\n",
    "            outdir=None\n",
    "        )\n",
    "        KEGG_df = enr_path.results\n",
    "\n",
    "        # Filter by overlap percentage and adjusted p-value\n",
    "        mask =  (KEGG_df[\"Adjusted P-value\"] < term_score_cap) & (KEGG_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > percentage))\n",
    "        filtered = KEGG_df[mask].copy()\n",
    "        \n",
    "        # Categorization from KEGG Level 2\n",
    "        filtered[\"KEGG_ID\"] = filtered[\"Term\"].str.replace(r\"\\s*-\\s*Homo sapiens.*$\", \"\", regex=True).str.lower().map(name_to_id)\n",
    "        filtered[\"Category\"] = filtered[\"KEGG_ID\"].map(get_kegg_level2)\n",
    "        \n",
    "        # Sort\n",
    "        filtered['Overlap (value)'] = filtered['Overlap'].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]))\n",
    "        filtered = filtered.sort_values(['Overlap (value)'], ascending=False)\n",
    "        \n",
    "        # Compute overlap score for every category:\n",
    "        filtered_exploded = filtered.explode('Category').reset_index(drop=True)\n",
    "        category_counts_and_overlap_score = {}\n",
    "        for val, group in filtered_exploded.groupby('Category'):\n",
    "            overlap_list = group[\"Overlap\"].tolist()\n",
    "            numerators = [(lambda x: int(x.split(\"/\")[0]))(e) for e in overlap_list]\n",
    "            denominators = [(lambda x: int(x.split(\"/\")[1]))(e) for e in overlap_list]\n",
    "            overlap_score = sum(numerators)/sum(denominators)\n",
    "            \n",
    "            category_counts_and_overlap_score[val] = (len(group),overlap_score,)\n",
    "        \n",
    "        category_counts_and_overlap_score_list[i] = category_counts_and_overlap_score\n",
    "        \n",
    "        # Add results to important terms\n",
    "        if not filtered.empty:\n",
    "            # print size of community\n",
    "            print(f\"Size of community: {len(community)}\")   \n",
    "            \n",
    "            # print number of filtered terms\n",
    "            print(f\"Number of filtered terms: {len(filtered)}\")\n",
    "            filtered.loc[:, \"Community Index\"] = i\n",
    "            filtered.loc[:, \"Community Size\"] = len(community)\n",
    "            important_terms = pd.concat([important_terms, filtered], ignore_index=True)\n",
    "            \n",
    "            # show results\n",
    "            display(HTML(filtered[[\"Community Index\",'Term','Overlap','Adjusted P-value',\"KEGG_ID\",\"Category\"]].head(10).to_html(max_cols=None)))\n",
    "            print(category_counts_and_overlap_score)\n",
    "            num_nonzero_communities += 1\n",
    "\n",
    "        i += 1\n",
    "    print(f\"{num_nonzero_communities} out of {len(communities)} communities had significant GO terms.\")\n",
    "    return important_terms,category_counts_and_overlap_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_important_terms,kegg_category_counts_and_overlap_score = kegg_enrichment(COMMUNITIES_HGNC,TERM_SCORE_CAP,PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_important_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Reactome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reactome enrichment\n",
    "def reactome_enrichment(communities,\n",
    "                        term_score_cap,\n",
    "                        percentage):\n",
    "    important_terms = pd.DataFrame(columns=[\"Community Index\",\"Community Size\",\"Term\", \"Overlap\", \"Adjusted P-value\",\"Category\"])\n",
    "    category_counts_and_overlap_score_list= {}\n",
    "    i = 0\n",
    "    num_nonzero_communities = 0\n",
    "    for community in communities:\n",
    "        enr_path = gp.enrichr(\n",
    "            gene_list=community,\n",
    "            gene_sets=['Reactome_2022'],\n",
    "            organism='Human',\n",
    "            outdir=None\n",
    "        )\n",
    "        Reactome_df = enr_path.results\n",
    "\n",
    "        # Filter by overlap percentage and adjusted p-value\n",
    "        mask =  (Reactome_df[\"Adjusted P-value\"] < term_score_cap) & (Reactome_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > percentage))\n",
    "        filtered = Reactome_df[mask].copy()\n",
    "        \n",
    "        # Categorization from Reactome Level 1\n",
    "        filtered[\"Category\"] = filtered[\"Term\"].str.extract(r\"(R-[A-Z]+-\\d+)\", expand=False).map(reactome_level1)\n",
    "        \n",
    "        # Sort\n",
    "        filtered['Overlap (value)'] = filtered['Overlap'].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]))\n",
    "        filtered = filtered.sort_values(['Overlap (value)'], ascending=False)\n",
    "        \n",
    "        # Compute overlap score for every category:\n",
    "        filtered_exploded = filtered.explode('Category').reset_index(drop=True)\n",
    "        category_counts_and_overlap_score = {}\n",
    "        for val, group in filtered_exploded.groupby('Category'):\n",
    "            overlap_list = group[\"Overlap\"].tolist()\n",
    "            numerators = [(lambda x: int(x.split(\"/\")[0]))(e) for e in overlap_list]\n",
    "            denominators = [(lambda x: int(x.split(\"/\")[1]))(e) for e in overlap_list]\n",
    "            overlap_score = sum(numerators)/sum(denominators)\n",
    "            \n",
    "            category_counts_and_overlap_score[val] = (len(group),overlap_score,)\n",
    "        \n",
    "        category_counts_and_overlap_score_list[i] = category_counts_and_overlap_score\n",
    "        \n",
    "        # Add results to important terms\n",
    "        if not filtered.empty:\n",
    "            print(f\"Size of community: {len(community)}\")\n",
    "            print(f\"Number of filtered terms: {len(filtered)}\")\n",
    "            filtered.loc[:, \"Community Index\"] = i\n",
    "            filtered.loc[:, \"Community Size\"] = len(community)\n",
    "            important_terms = pd.concat([important_terms, filtered], ignore_index=True)\n",
    "            display(HTML(filtered[[\"Community Index\",'Term','Overlap','Adjusted P-value',\"Category\"]].head(30).to_html(max_cols=None)))\n",
    "            print(category_counts_and_overlap_score)\n",
    "            num_nonzero_communities += 1\n",
    "        i += 1\n",
    "    print(f\"{num_nonzero_communities} out of {len(communities)} communities had significant GO terms.\")\n",
    "    return important_terms,category_counts_and_overlap_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactome_important_terms,reactome_category_counts_and_overlap_score = reactome_enrichment(COMMUNITIES_HGNC,TERM_SCORE_CAP,PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactome_important_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Disease Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disease_term_score_cap = 0.001\n",
    "# disease_percentage = 0.1\n",
    "# important_diseases = pd.DataFrame(columns=[\"Community Index\",\"Community Size\",\"Term\", \"Overlap\", \"Adjusted P-value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Disease-gene enrichment libraries\n",
    "# disease_sets = [\n",
    "#     'DisGeNET_2020', # curated geneâ€“disease associations\n",
    "#     'GWAS_Catalog_2023', # genome-wide association hits\n",
    "#     'OMIM_Disease', # Mendelian disorders\n",
    "#     'Jensen_DISEASES' # text-mined associations\n",
    "# ]\n",
    "\n",
    "# # # Disease-gene enrichment Analysis; save terms with small size and high p-value\n",
    "# i = 0\n",
    "# for community in communities_HGNC:\n",
    "#     # Gene Ontology enrichment\n",
    "#     enr_disease = gp.enrichr(\n",
    "#         gene_list=community,\n",
    "#         gene_sets=disease_sets,\n",
    "#         organism='Human',\n",
    "#         outdir=None # don't write to disk\n",
    "#     )\n",
    "#     enr_disease_df = enr_disease.results.sort_values('Adjusted P-value')\n",
    "#     print(f\"Size of community: {len(community)}\")\n",
    "\n",
    "#     mask =  (enr_disease_df[\"Adjusted P-value\"] < disease_term_score_cap) & (enr_disease_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > disease_percentage))\n",
    "        \n",
    "#     filtered = enr_disease_df[mask].copy()\n",
    "#     if not filtered.empty:\n",
    "#         filtered.loc[:, \"Community Index\"] = i\n",
    "#         filtered.loc[:, \"Community Size\"] = len(community)\n",
    "#         important_diseases = pd.concat([important_diseases, filtered], ignore_index=True)\n",
    "\n",
    "#     display(HTML(filtered[['Term','Overlap','Adjusted P-value']].head(10).to_html(max_cols=None)))\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# Important Terms Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Constructing Important Terms df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comm_similarity_with_term(x,y):\n",
    "    return 1-(abs(x-y)/max(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_terms = pd.DataFrame(columns=[\"Community Index\",\"Community Size\",\"Term\", \"Overlap\", \"Adjusted P-value\",\"Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [go_important_terms,kegg_important_terms,reactome_important_terms]\n",
    "important_terms = pd.concat(c, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important_terms = important_terms.sort_values(by=\"Overlap (value)\",ascending=False)\n",
    "important_terms = important_terms.sort_values(by=\"Community Index\")\n",
    "important_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community id to size dict\n",
    "com_id_to_size = {i : len(COMMUNITIES_HGNC[i]) for i in range(len(COMMUNITIES_HGNC))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_com_id_to_size = important_terms.drop_duplicates(subset=\"Community Index\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_size_dict = dict(zip(unique_com_id_to_size[\"Community Index\"], unique_com_id_to_size[\"Community Size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_terms.to_csv(f\"../output/{DISEASE}/important_terms_{DISEASE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### Graph Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df must have: \"Community Index\", \"Term\", \"Overlap (value)\"\n",
    "work = important_terms.loc[:, [\"Community Index\", \"Term\", \"Overlap (value)\",\"Category\"]].copy()\n",
    "work[\"Overlap (value)\"] = work[\"Overlap (value)\"].astype(float)\n",
    "\n",
    "# Ensure (community, term) uniqueness\n",
    "dupes = work.duplicated(subset=[\"Community Index\", \"Term\"], keep=False)\n",
    "if dupes.any():\n",
    "    raise ValueError(\"Duplicated (Community Index, Term) rows found; ensure uniqueness first.\")\n",
    "\n",
    "# --- Build edge weights AND collect contributing terms per pair ---\n",
    "edge_weights = {}              # (u, v) -> float\n",
    "edge_counts  = {}              # (u, v) -> int\n",
    "edge_terms   = {}              # (u, v) -> list[(term, contrib)]\n",
    "\n",
    "for term, sub in work.groupby(\"Term\", sort=False):\n",
    "    comms  = sub[\"Community Index\"].to_numpy()\n",
    "    scores = sub[\"Overlap (value)\"].to_numpy()\n",
    "    if len(comms) < 2:\n",
    "        continue\n",
    "    for i, j in combinations(range(len(comms)), 2):\n",
    "        u, v = comms[i], comms[j]\n",
    "        if u > v: u, v = v, u  # canonical ordering\n",
    "        contrib = comm_similarity_with_term(scores[i], scores[j])\n",
    "\n",
    "        edge_weights[(u, v)] = edge_weights.get((u, v), 0.0) + contrib\n",
    "        edge_counts[(u, v)]  = edge_counts.get((u, v), 0)    + 1\n",
    "        edge_terms.setdefault((u, v), []).append((term, contrib))\n",
    "\n",
    "# Sort contributing terms by contribution desc for each edge\n",
    "for key in edge_terms:\n",
    "    edge_terms[key].sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "# --- Build edge list DataFrame (optional, useful to inspect) ---\n",
    "edge_df = pd.DataFrame(\n",
    "    [(u, v, edge_weights[(u, v)], edge_counts[(u, v)], edge_terms.get((u, v), []))\n",
    "     for (u, v) in edge_weights.keys()],\n",
    "    columns=[\"u\", \"v\", \"weight\", \"shared_terms\", \"terms_contrib\"]\n",
    ").sort_values([\"weight\", \"shared_terms\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "# --- Build NetworkX graph with attributes ---\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(pd.unique(work[\"Community Index\"]))\n",
    "for _, r in edge_df.iterrows():\n",
    "    G.add_edge(\n",
    "        int(r.u), int(r.v),\n",
    "        weight=float(r.weight),\n",
    "        shared_terms=int(r.shared_terms),\n",
    "        terms_contrib=r.terms_contrib  # list of (term, contrib) sorted desc\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------Table------------------\n",
    "term_contribs = []\n",
    "\n",
    "for term, sub in work.groupby(\"Term\", sort=False):\n",
    "    comms  = sub[\"Community Index\"].to_numpy()\n",
    "    scores = sub[\"Overlap (value)\"].to_numpy()\n",
    "    if len(comms) < 2:\n",
    "        continue\n",
    "    for i, j in combinations(range(len(comms)), 2):\n",
    "        u, v = comms[i], comms[j]\n",
    "        if u > v:\n",
    "            u, v = v, u\n",
    "        contrib = comm_similarity_with_term(scores[i], scores[j])\n",
    "        cat = sub[\"Category\"].iloc[0] if \"Category\" in sub.columns else None\n",
    "        term_contribs.append((u, v, term, contrib, cat))\n",
    "\n",
    "# 2) Build DataFrame\n",
    "term_df = pd.DataFrame(term_contribs, columns=[\"u\", \"v\", \"Term\", \"Contribution\",\"Category\"])\n",
    "# 3) Sort and aggregate terms per edge (keep per-term order)\n",
    "agg_blocks = []\n",
    "for (u, v), sub in term_df.groupby([\"u\", \"v\"]):\n",
    "    sub_sorted = sub.sort_values(\"Contribution\", ascending=False)\n",
    "    \n",
    "    # Create category count dictionary\n",
    "    category_counts = Counter(\n",
    "        c\n",
    "        for cats in sub_sorted[\"Category\"].dropna()\n",
    "        for c in cats\n",
    "    )\n",
    "    category_counts_dict = dict(category_counts)\n",
    "\n",
    "    # sub_sorted = sub.sort_values(sub_sorted[\"Category\"].apply(tuple), ascending=False)\n",
    "    block = \"\\n\".join(\n",
    "        [f\"  - {t} {cat} ({c:.3f})\"\n",
    "        for t, c, cat in zip(sub_sorted[\"Term\"], sub_sorted[\"Contribution\"], sub_sorted[\"Category\"])]\n",
    "    )\n",
    "    total = sub_sorted[\"Contribution\"].sum()\n",
    "    agg_blocks.append({\n",
    "        \"u\": u,\n",
    "        \"v\": v,\n",
    "        \"Total Weight\": total,\n",
    "        \"Terms (by contribution)\": block,\n",
    "        \"Category Count\": category_counts_dict\n",
    "    })\n",
    "\n",
    "# 4) Create final block table\n",
    "block_df = pd.DataFrame(agg_blocks).sort_values(\"Total Weight\", ascending=False).reset_index(drop=True)\n",
    "# 5) Display nicely\n",
    "for _, row in block_df.iterrows():\n",
    "    print(f\"Community pair ({row.u}, {row.v}) â€” Total Weight = {row['Total Weight']:.3f}\")\n",
    "    print(row[\"Terms (by contribution)\"])\n",
    "    \n",
    "    print()\n",
    "    print(\"Category Count:\")\n",
    "    for key, value in sorted(row[\"Category Count\"].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_df.to_excel(\"output.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Category Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_category_count_by_comm(category_count_by_comm):\n",
    "    for comm_id, cat_dict in category_count_by_comm.items():\n",
    "        print(f\"\\nðŸ§© Community {comm_id}\")\n",
    "        print(\"-\" * (14 + len(str(comm_id))))\n",
    "\n",
    "        if not cat_dict:\n",
    "            print(\"  (no categories)\")\n",
    "            continue\n",
    "\n",
    "        # Sort categories by descending count\n",
    "        for cat, count in sorted(cat_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  â€¢ {cat:<50} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count_by_comm = {}\n",
    "for i in range(num_selected_comm):\n",
    "    comm_cates = go_category_counts_and_overlap_score[i] | kegg_category_counts_and_overlap_score[i] | reactome_category_counts_and_overlap_score[i]\n",
    "    category_count_by_comm[i] = dict(sorted(comm_cates.items(), key=lambda x: x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_count_by_comm(category_count_by_comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "# Robustness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_enrichment_func(community,term_score_cap,percentage):\n",
    "    # GO df\n",
    "    enr_go = gp.enrichr(\n",
    "        gene_list=community,\n",
    "        gene_sets=['GO_Biological_Process_2023',\n",
    "                'GO_Molecular_Function_2023',\n",
    "                'GO_Cellular_Component_2023'],\n",
    "        organism='Human',\n",
    "        outdir=None # don't write to disk\n",
    "    )\n",
    "    GO_df = enr_go.results\n",
    "    mask =  (GO_df[\"Adjusted P-value\"] < term_score_cap) & (GO_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > percentage))\n",
    "    GO_df = GO_df[mask].copy()   \n",
    "    \n",
    "    # KEGG df\n",
    "    enr_kegg = gp.enrichr(\n",
    "        gene_list=community,\n",
    "        gene_sets=['KEGG_2021_Human'],\n",
    "        organism='Human',\n",
    "        outdir=None\n",
    "    )\n",
    "    KEGG_df = enr_kegg.results\n",
    "    mask =  (KEGG_df[\"Adjusted P-value\"] < term_score_cap) & (KEGG_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > percentage))\n",
    "    KEGG_df = KEGG_df[mask].copy() \n",
    "       \n",
    "    # Reactome df\n",
    "    enr_reactome = gp.enrichr(\n",
    "        gene_list=community,\n",
    "        gene_sets=['Reactome_2022'],\n",
    "        organism='Human',\n",
    "        outdir=None\n",
    "    )\n",
    "    Reactome_df = enr_reactome.results  \n",
    "    mask =  (Reactome_df[\"Adjusted P-value\"] < term_score_cap) & (Reactome_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > percentage))\n",
    "    Reactome_df = Reactome_df[mask].copy()\n",
    "    \n",
    "    \n",
    "    all_df = [GO_df,KEGG_df,Reactome_df]\n",
    "    # build result df by concatenating\n",
    "    result = pd.concat(all_df, ignore_index=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONDecodeError\n",
    "\n",
    "# ---------------- 1) Safe wrapper that calls YOUR enrichr function ----------------\n",
    "_ENR_CACHE = {}  # key: tuple(sorted(genes)) -> DataFrame (copy)\n",
    "\n",
    "def run_enrichment_safe(run_enrichment_func, community, retries=5, base_sleep=0.8):\n",
    "    \"\"\"\n",
    "    Calls user's run_enrichment_func(community) with retries + memoization.\n",
    "    Returns a DataFrame (possibly empty). Never raises JSONDecodeError outward.\n",
    "    \"\"\"\n",
    "    # Ensure we always pass a list of gene symbols (never a bare string)\n",
    "    genes = np.atleast_1d(np.array(community, dtype=object)).tolist()\n",
    "    if len(genes) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    key = tuple(sorted(genes))\n",
    "    if key in _ENR_CACHE:\n",
    "        return _ENR_CACHE[key].copy()\n",
    "\n",
    "    for a in range(retries):\n",
    "        try:\n",
    "            df = run_enrichment_func(genes,TERM_SCORE_CAP,PERCENTAGE)\n",
    "            if df is None:\n",
    "                # treat as transient failure to trigger retry\n",
    "                raise RuntimeError(\"run_enrichment_func returned None\")\n",
    "            _ENR_CACHE[key] = df.copy()\n",
    "            return df\n",
    "        except (JSONDecodeError, OSError, RuntimeError, ValueError) as e:\n",
    "            # Transient errors from HTTP/JSON/file handling inside gseapy\n",
    "            if a == retries - 1:\n",
    "                # Give up: return empty so pipeline continues\n",
    "                return pd.DataFrame()\n",
    "            time.sleep(base_sleep * (2 ** a) + np.random.rand() * 0.3)\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# ---------------- 2) Minimal bootstrap to record robust terms ----------------\n",
    "def get_robust_terms(communities_HGNC, run_enrichment_func,\n",
    "                     R=50, leaveout=0.10, recurrence_cutoff=0.70, seed=42):\n",
    "    \"\"\"\n",
    "    Uses YOUR run_enrichment_func(community)->DataFrame (already filtered to significant terms).\n",
    "    Returns DataFrame with columns: community_id, term, recurrence (and Gene_set if available).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "\n",
    "    for cid, community in enumerate(communities_HGNC):\n",
    "        n = len(community)\n",
    "        if n == 0:\n",
    "            continue\n",
    "        drop_k = max(1, int(np.floor(leaveout * n)))\n",
    "        counts = Counter()\n",
    "\n",
    "        for _ in range(R):\n",
    "            # Jackknife subset (ensure not empty)\n",
    "            keep = np.ones(n, dtype=bool)\n",
    "            keep[rng.choice(n, size=min(drop_k, n), replace=False)] = False\n",
    "            sub = np.atleast_1d(np.array(community, dtype=object)[keep]).tolist()\n",
    "            if len(sub) == 0:\n",
    "                continue\n",
    "\n",
    "            df = run_enrichment_safe(run_enrichment_func, sub)\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "\n",
    "            # Your function already returns significant terms; just count them.\n",
    "            # If it includes multiple libraries, preserve Gene_set to disambiguate names.\n",
    "            if 'Term' not in df.columns:\n",
    "                continue  # be defensive\n",
    "\n",
    "            if 'Gene_set' in df.columns:\n",
    "                terms = (df[['Term', 'Gene_set']]\n",
    "                         .dropna()\n",
    "                         .drop_duplicates()\n",
    "                         .apply(lambda r: f\"{r['Term']}|{r['Gene_set']}\", axis=1)\n",
    "                         .tolist())\n",
    "            else:\n",
    "                terms = df['Term'].dropna().drop_duplicates().tolist()\n",
    "\n",
    "            counts.update(terms)\n",
    "\n",
    "            # tiny pause helps with API rate limits if your func calls Enrichr internally\n",
    "            time.sleep(0.03)\n",
    "\n",
    "        # Keep only robust terms\n",
    "        for t, c in counts.items():\n",
    "            freq = c / max(R, 1)\n",
    "            if freq >= recurrence_cutoff:\n",
    "                if '|' in t:\n",
    "                    term, gene_set = t.split('|', 1)\n",
    "                    rows.append({'Community Index': cid, 'Term': term, 'recurrence': freq, 'Gene_set': gene_set})\n",
    "                else:\n",
    "                    rows.append({'Community Index': cid, 'Term': t, 'recurrence': freq})\n",
    "\n",
    "    return (pd.DataFrame(rows)\n",
    "              .sort_values(['Community Index', 'recurrence'], ascending=[True, False])\n",
    "              .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "twr3 = get_robust_terms([COMMUNITIES_HGNC[1]], run_enrichment_func,\n",
    "                                R=25, leaveout=0.1, recurrence_cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "twr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_with_recurrence = get_robust_terms(COMMUNITIES_HGNC, run_enrichment_func,\n",
    "                                R=10, leaveout=0.1, recurrence_cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_with_recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename important terms to match terms_with_recurrence\n",
    "important_terms = important_terms.rename(columns={'index': 'community_id'})\n",
    "important_terms = important_terms.rename(columns={'Term': 'term'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_with_rec_merged = important_terms.merge(\n",
    "    terms_with_recurrence[['community_id', 'term', 'Gene_set', 'recurrence']],\n",
    "    on=['community_id', 'term', 'Gene_set'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "terms_with_rec_merged['recurrence'] = terms_with_rec_merged['recurrence'].fillna(0.0)\n",
    "\n",
    "terms_with_rec_merged = terms_with_rec_merged.sort_values(\n",
    "    ['community_id', 'recurrence'],\n",
    "    ascending=[True, False]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_with_rec_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_summary = (\n",
    "    terms_with_rec_merged\n",
    "    .groupby(\"community_id\")[\"recurrence\"]\n",
    "    .agg(mean_recurrence=\"mean\", term_count=\"count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(community_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(terms_with_recurrence.to_html(max_cols=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "# Checks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in communities:\n",
    "    print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGIDB_genes_ncbi = list(DGIDB_gene_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comms_ncbi = index_to_ncbi(communities,index_to_gene_distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_comms_ncbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGIDB_count(c):\n",
    "    return len(set(c) & set(DGIDB_genes_ncbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in all_comms_ncbi:\n",
    "    print(len(c),DGIDB_count(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tbd(id):\n",
    "    print(len(communities[id]))\n",
    "    c8_ncbi = index_to_ncbi([communities[id]])[0]\n",
    "    print(len(c8_ncbi))\n",
    "    print(DGIDB_count(c8_ncbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tbd_selected(id):\n",
    "    print(len(communities_selected[id]))\n",
    "    c8_ncbi = index_to_ncbi([communities_selected[id]])[0]\n",
    "    print(len(c8_ncbi))\n",
    "    print(DGIDB_count(c8_ncbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbd_selected(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
